{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner import RandomSearch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from kerastuner.tuners import RandomSearch\n",
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./merged_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['type']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.replace({0: 1, 1: 0, 2: 0,3:0},inplace=True)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 1 with \"Fire\" and 0 with \"No Fire\" \n",
    "target.replace({1: \"Fire\", 0: \"No Fire\"},inplace=True)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add gap between bars and add number on top of bars\n",
    "sns_hist = sns.histplot(target, stat='count', discrete=True)\n",
    "plt.title('Distribution of Target Variable') \n",
    "plt.xlabel('Type')\n",
    "\n",
    "colors = ['red', 'blue']\n",
    "for i, bar in enumerate(sns_hist.patches):\n",
    "    bar.set_color(colors[i])\n",
    "    bar.set_edgecolor('black') \n",
    "    bar.set_linewidth(1)\n",
    "\n",
    "sns_hist.bar_label(sns_hist.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Data Preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('./merged_data.csv')#\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = data['acq_time']\n",
    "\n",
    "def getCosSinHour(df):\n",
    "\n",
    "    date = pd.to_datetime(data['acq_date'] +'T'+ data['acq_time'])\n",
    "\n",
    "    hour = date.dt.hour\n",
    "\n",
    "    cos_hour = np.cos(2 * np.pi * hour/24)\n",
    "    sin_hour = np.sin(2 * np.pi * hour/24)\n",
    "\n",
    "    return cos_hour, sin_hour\n",
    "\n",
    "cos_hour, sin_hour = getCosSinHour(data)\n",
    "# data['cos_hour'] = cos_hour\n",
    "# data['sin_hour'] = sin_hour\n",
    "\n",
    "#convert daynight to 1 and 0 and drop unwanted rows in that column\n",
    "#convert every element in daynight to int is possible\n",
    "\n",
    "\n",
    "#convert every element in daynight to int is possible\n",
    "data['daynight'] = data['daynight'].apply(lambda x: int(x) if x.isdigit() else x)\n",
    "\n",
    "data['daynight'] = data['daynight'].replace('D', 1)\n",
    "data['daynight'] = data['daynight'].replace('N', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['type']\n",
    "dropped_data = data.drop(['longitude','latitude','acq_date','acq_time','satellite','instrument','type','confidence','version','precipitation_sum','frp'], axis=1)#,'daynight'\n",
    "dropped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.replace({0: 1, 1: 0, 2: 0,3:0},inplace=True)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_train_reshaped = np.expand_dims(dropped_data.values, axis=1)\n",
    "\n",
    "\n",
    "# n_samples, timesteps, n_features = x_train_reshaped.shape\n",
    "# x_train_reshaped_2d = x_train_reshaped.reshape(n_samples, timesteps * n_features)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "x_train_balanced, y_train_balanced = smote.fit_resample(dropped_data, target)\n",
    "\n",
    "# Reshape back to 3D\n",
    "#x_train_balanced = x_train_balanced.reshape(-1, timesteps, n_features)\n",
    "\n",
    "# Verify the new shape\n",
    "print(\"Balanced x_train shape:\", x_train_balanced.shape)\n",
    "print(\"Balanced y_train shape:\", y_train_balanced.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_balanced, y_train_balanced, test_size=0.2, random_state=42,stratify=y_train_balanced)\n",
    "x_train,x_val,y_train, y_val = train_test_split(x_train,y_train, test_size=0.2, random_state=42,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dropped_data,target, test_size=0.2, random_state=42,stratify=target)\n",
    "x_train,x_val,y_train, y_val = train_test_split(x_train,y_train, test_size=0.2, random_state=42,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the scalers\n",
    "scalers = {}\n",
    "\n",
    "for column in x_train.columns:\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train[[column]] = scaler.fit_transform(x_train[[column]])\n",
    "    scalers[column] = scaler\n",
    "\n",
    "for column, scaler in scalers.items():\n",
    "    joblib.dump(scaler, f\"{column}_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved scalers\n",
    "scalers = {}\n",
    "for column in x_train.columns:\n",
    "\n",
    "    if column == 'daynight':\n",
    "        continue\n",
    "    scaler = joblib.load(f\"{column}_scaler.pkl\")\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# Transform the validation and test features using the loaded scalers\n",
    "x_val_scaled = x_val.copy()  \n",
    "x_train_scaled = x_train.copy()\n",
    "x_test_scaled = x_test.copy()  \n",
    "for column, scaler in scalers.items():\n",
    "    x_train_scaled[[column]] = scaler.transform(x_train[[column]])\n",
    "    x_val_scaled[[column]] = scaler.transform(x_val[[column]])\n",
    "    x_test_scaled[[column]] = scaler.transform(x_test[[column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Model Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__#'1.21.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Iv numpy==1.19.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM\n",
    "def reshape_data_for_lstm(x_train, x_test_scaled, x_val_scaled):\n",
    "    x_train_array = x_train.values\n",
    "    x_train_reshaped = np.expand_dims(x_train_array, axis=1)\n",
    "\n",
    "    x_test_array = x_test_scaled.values\n",
    "    x_test_reshaped = np.expand_dims(x_test_array, axis=1)\n",
    "\n",
    "    x_val_array = x_val_scaled.values\n",
    "    x_val_reshaped = np.expand_dims(x_val_array, axis=1)\n",
    "\n",
    "    input_shape = (1, x_train_reshaped.shape[2]) #for LSTM\n",
    "    return x_train_reshaped, x_test_reshaped, x_val_reshaped, input_shape\n",
    "\n",
    "\n",
    "\n",
    "###cnn\n",
    "def reshape_data_for_cnn(x_train, x_test_scaled, x_val_scaled):\n",
    "    x_train_array = x_train.values\n",
    "    x_train_reshaped = np.expand_dims(x_train_array, axis=-1)\n",
    "\n",
    "    x_test_array = x_test_scaled.values\n",
    "    x_test_reshaped = np.expand_dims(x_test_array, axis=-1)\n",
    "\n",
    "    x_val_array = x_val_scaled.values\n",
    "    x_val_reshaped = np.expand_dims(x_val_array, axis=-1)\n",
    "\n",
    "    input_shape = (x_train_reshaped.shape[1],1) #cnn\n",
    "    return x_train_reshaped, x_test_reshaped, x_val_reshaped, input_shape\n",
    "\n",
    "\n",
    "x_train_reshaped_lstm, x_test_reshaped_lstm, x_val_reshaped_lstm, input_shape_lstm = reshape_data_for_lstm(x_train_scaled,  x_test_scaled, x_val_scaled)\n",
    "\n",
    "x_train_reshaped, x_test_reshaped, x_val_reshaped, input_shape = reshape_data_for_cnn(x_train_scaled, x_test_scaled, x_val_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#lstm\n",
    "def build_model_lstm(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('input_units', min_value=32, max_value=512, step=32),\n",
    "                   return_sequences=True,\n",
    "                   input_shape=input_shape_lstm)) \n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(LSTM(units=hp.Int(f'lstm_{i}_units', min_value=32, max_value=512, step=32), return_sequences=True,kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-6, max_value=1e-2, sampling='log'))))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(units=hp.Int('layer_2_neurons', min_value=32, max_value=512, step=32), return_sequences=True,kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-6, max_value=1e-2, sampling='log'))))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0, max_value=0.5, step=0.1)))\n",
    "    #model.add(Dense(units=y_train.shape[1], activation=hp.Choice('dense_activation', values=['relu', 'sigmoid'], default='relu')))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "#cnn\n",
    "def build_model_cnn(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    model.add(Conv1D(filters=hp.Int('input_filters', min_value=32, max_value=512, step=32),\n",
    "                     kernel_size=1,#hp.Int('input_kernel_size', min_value=1, max_value=7, step=2),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape,\n",
    "                     kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-6, max_value=1e-2, sampling='log'))))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Additional Conv1D layers based on n_layers\n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(Conv1D(filters=hp.Int(f'conv_{i}_filters', min_value=32, max_value=512, step=32),\n",
    "                         kernel_size=1,#hp.Int(f'conv_{i}_kernel_size', min_value=3, max_value=7, step=2),\n",
    "                         activation='relu',\n",
    "                         kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-6, max_value=1e-2, sampling='log'))))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # Final Conv1D layer\n",
    "    model.add(Conv1D(filters=hp.Int('final_conv_filters', min_value=32, max_value=512, step=32),\n",
    "                     kernel_size=1,#p.Int('final_conv_kernel_size', min_value=3, max_value=7, step=2),\n",
    "                     activation='relu',\n",
    "                     kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-6, max_value=1e-2, sampling='log'))))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Flatten the output before passing it to Dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layer for binary classification\n",
    "    model.add(Dense(units=1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # Monitor the validation loss\n",
    "    patience=3, # Stop after 5 epochs with no improvement\n",
    "    mode='min', # Minimize the loss\n",
    "    restore_best_weights=True # Restore the weights from the epoch with the best validation loss\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the Keras Tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model_lstm,\n",
    "    objective='val_loss',\n",
    "    max_trials=4,\n",
    "    directory='./tuner',\n",
    "    project_name='/lstm',\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    x=x_train_reshaped_lstm,\n",
    "    y=y_train.values.reshape(-1,1),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test_reshaped_lstm, y_test.values.reshape(-1, 1)),\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn1 = load_model('models/cnn_1rst_88acc.h5')\n",
    "model_cnn2 = load_model('models/cnn_daynight_89acc.h5')\n",
    "model_lstm1 = load_model('models/lstm_model_89acc.h5')\n",
    "model_lstm2 = load_model('models/lstm_daynight_90acc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner.get_best_models(num_models=1)[0]\n",
    "tuner.get_best_models(num_models=1)[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner.get_best_models(num_models=1)[0]\n",
    "tuner.get_best_models(num_models=1)[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner.get_best_models(num_models=1)[0]\n",
    "tuner.get_best_models(num_models=1)[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner.get_best_models(num_models=1)[0]  # Specify the number of best models you want to retrieve\n",
    "\n",
    "# Print the summary of the best model and store the model weigths to use for prediction\n",
    "# model = 0\n",
    "# for i in best_models:\n",
    "#     print(i.summary())\n",
    "#     model = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = best_models.evaluate(x_val_reshaped, y_val.values.reshape(-1,1))\n",
    "\n",
    "print(\"loss:\", inference[0])\n",
    "print(\"accuracy:\", inference[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_models.predict_classes(x_val_reshaped)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix( y_val.values.reshape(-1,1), y_pred.reshape(-1,1))# for lstm y_val.values.reshape(-1,1), y_pred.reshape(-1,1) #for cnn y_val, y_pred\n",
    " \n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', annot_kws={\"size\": 12})\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "\n",
    "y_pred_prob = best_models.predict(x_test_reshaped).ravel()\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "y_pred_test = best_models.predict_classes(x_val_reshaped).reshape(-1,1)\n",
    "\n",
    "precision = precision_score(y_val, y_pred_test)\n",
    "recall = recall_score(y_val, y_pred_test)\n",
    "f1 = f1_score(y_val, y_pred_test)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_test))\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Ensemble Prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# import traintestsplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predDataForModelWithDaynight(mode):\n",
    "    data = pd.read_csv(r'./merged_data.csv')\n",
    "\n",
    "    data['daynight'] = data['daynight'].replace('D', 1)\n",
    "    data['daynight'] = data['daynight'].replace('N', 0)\n",
    "    target = data['type']\n",
    "\n",
    "    if mode == 'daynight':\n",
    "        dropped_data = data.drop(['longitude','latitude','acq_date','acq_time','satellite','instrument','type','confidence','version','precipitation_sum','frp'], axis=1)#,'daynight'\n",
    "    else:\n",
    "        dropped_data = data.drop(['longitude','latitude','acq_date','acq_time','satellite','instrument','type','daynight','confidence','version','precipitation_sum','frp','daynight'], axis=1)\n",
    "\n",
    "    target.replace({0: 1, 1: 0, 2: 0,3:0},inplace=True)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dropped_data,target, test_size=0.2, random_state=42,stratify=target)\n",
    "    x_train,x_val,y_train, y_val = train_test_split(x_train,y_train, test_size=0.2, random_state=42,stratify=y_train)\n",
    "\n",
    "    # Load the saved scalers\n",
    "    scalers = {}\n",
    "    for column in x_train.columns:\n",
    "\n",
    "        if column == 'daynight':\n",
    "            continue\n",
    "        scaler = joblib.load(f\"{column}_scaler.pkl\")\n",
    "        scalers[column] = scaler\n",
    "\n",
    "    # Transform the validation and test features using the loaded scalers\n",
    "    x_val_scaled = x_val.copy()\n",
    "    x_train_scaled = x_train.copy()\n",
    "    x_test_scaled = x_test.copy()\n",
    "    for column, scaler in scalers.items():\n",
    "        x_train_scaled[[column]] = scaler.transform(x_train[[column]])\n",
    "        x_val_scaled[[column]] = scaler.transform(x_val[[column]])\n",
    "        x_test_scaled[[column]] = scaler.transform(x_test[[column]])\n",
    "\n",
    "    x_train_reshaped, x_test_reshaped, x_val_reshaped, input_shape = reshape_data_for_cnn(x_train_scaled, x_test_scaled, x_val_scaled)\n",
    "    x_train_reshaped_lstm, x_test_reshaped_lstm, x_val_reshaped_lstm, input_shape = reshape_data_for_lstm(x_train_scaled, x_test_scaled, x_val_scaled)\n",
    "\n",
    "    return x_val_reshaped, x_val_reshaped_lstm, y_val\n",
    "\n",
    "x_val_reshaped_daynight, x_val_reshaped_lstm_daynight, y_val = predDataForModelWithDaynight(mode='daynight')\n",
    "x_val_reshaped, x_val_reshaped_lstm, y_val = predDataForModelWithDaynight(mode='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_reshaped_daynight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_reshaped_daynight = np.asarray(x_val_reshaped_daynight).astype('float32')\n",
    "x_val_reshaped_lstm_daynight = np.asarray(x_val_reshaped_lstm_daynight).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_1 = load_model(r'F:\\code\\cnn_1rst_88acc.h5')\n",
    "model_2 = load_model(r'F:\\code\\cnn_daynight_89acc.h5')\n",
    "model_3 = load_model(r'F:\\code\\lstm_model_89acc.h5')\n",
    "model_4 = load_model(r'F:\\code\\lstm_daynight_90acc.h5')\n",
    "\n",
    "#perform majority voting\n",
    "y_pred_1 = model_1.predict_classes(x_val_reshaped)\n",
    "y_pred_2 = model_2.predict_classes(x_val_reshaped_daynight)\n",
    "y_pred_3 = model_3.predict_classes(x_val_reshaped_lstm)\n",
    "y_pred_4 = model_4.predict_classes(x_val_reshaped_lstm_daynight)\n",
    "\n",
    "#perform majority voting for the ensemble model while outputs are classes\n",
    "\n",
    "\n",
    "# #give classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_val, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def vote(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    " \n",
    "    return num\n",
    "\n",
    "y_ens_pred = []\n",
    "count = []\n",
    "for i in range(len(y_pred_1)):\n",
    "    count.append(y_pred_1[i])\n",
    "    #count.append(y_pred_2[i])\n",
    "    count.append(y_pred_3[i])\n",
    "    count.append(y_pred_4[i])\n",
    "    y_ens_pred.append(vote(count))\n",
    "    count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: model 1\")\n",
    "y_pred_1 = [int(x) for x in y_pred_1]\n",
    "print(classification_report(y_val, y_pred_1))\n",
    "\n",
    "print(\"Classification Report: model 2\")\n",
    "y_pred_2 = [int(x) for x in y_pred_2]\n",
    "print(classification_report(y_val, y_pred_2))\n",
    "\n",
    "print(\"Classification Report: model 3\")\n",
    "y_pred_3 = [int(x) for x in y_pred_3]\n",
    "print(classification_report(y_val, y_pred_3))\n",
    "\n",
    "print(\"Classification Report: model 4\")\n",
    "y_pred_4 = [int(x) for x in y_pred_4]\n",
    "print(classification_report(y_val, y_pred_4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#give classification report\n",
    "print(\"Classification Report: 2,3,4\")\n",
    "print(classification_report(y_val, y_ens_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Mixture Of Experts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "from kerastuner import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_expert_model_cnn(input_shape,num_of_hidden):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=1,\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape,\n",
    "                     kernel_regularizer=l2(1e-2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Additional Conv1D layers\n",
    "    for i in range(num_of_hidden):\n",
    "        model.add(Conv1D(filters=64,\n",
    "                         kernel_size=1,\n",
    "                         activation='relu',\n",
    "                         kernel_regularizer=l2(1e-2)))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    # Final Conv1D layer\n",
    "    model.add(Conv1D(filters=64,\n",
    "                     kernel_size=1,\n",
    "                     activation='relu',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    \n",
    "    # Flatten the output before passing it to Dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layer for binary classification\n",
    "    model.add(Dense(units=1, activation='softmax'))  # Assuming binary classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "#create similar lstm model\n",
    "def build_expert_model_lstm(input_shape,num_of_hidden):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64,\n",
    "                   return_sequences=True,\n",
    "                   input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for i in range(num_of_hidden):\n",
    "        model.add(LSTM(units=64, return_sequences=True,kernel_regularizer=l2(1e-4)))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(units=64, return_sequences=True,kernel_regularizer=l2(1e-4)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    #add flatten layer \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=1, activation='softmax'))  # Assuming binary classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "# expert1 = build_expert_model_cnn(input_shape,3)\n",
    "# expert2 = build_expert_model_cnn(input_shape,4)\n",
    "expert1 = build_expert_model_lstm(input_shape_lstm,5) \n",
    "expert3 = build_expert_model_lstm(input_shape_lstm,3)\n",
    "expert4 = build_expert_model_lstm(input_shape_lstm,4)\n",
    "experts = [expert1,  expert3, expert4]#expert2,\n",
    "\n",
    "#to kalytero einai 3 cnn me 3,4,5 num of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gating_network(input_shape, num_experts):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(64, activation='relu',kernel_regularizer=l2(1e-2))(x)\n",
    "    #x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu',kernel_regularizer=l2(1e-2))(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_experts, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "num_experts = len(experts)+1\n",
    "gating_network = build_gating_network(input_shape_lstm, num_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_of_experts(inputs, experts, gating_network):\n",
    "    gating_weights = gating_network(inputs)\n",
    "    \n",
    "    expert_outputs = [expert(inputs) for expert in experts]\n",
    "    expert_outputs = tf.stack(expert_outputs, axis=-1)\n",
    "    \n",
    "    \n",
    "    weighted_expert_outputs = tf.reduce_sum(expert_outputs * tf.expand_dims(gating_weights, -1), axis=-1)\n",
    "    \n",
    "    return weighted_expert_outputs\n",
    "\n",
    "inputs = Input(shape=input_shape_lstm)\n",
    "outputs = mixture_of_experts(inputs, experts, gating_network)\n",
    "moe_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile and train the model\n",
    "moe_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam',learning_rate=2e-4, metrics=['accuracy'])\n",
    "moe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # Monitor the validation loss\n",
    "    patience=3, # Stop after 5 epochs with no improvement\n",
    "    mode='min', # Minimize the loss\n",
    "    restore_best_weights=True # Restore the weights from the epoch with the best validation loss\n",
    ")\n",
    "#add reducelr on plateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.05,\n",
    "                              patience=3, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Set up the Keras Tuner\n",
    "# tuner = RandomSearch(\n",
    "#     build_model_lstm,\n",
    "#     objective='val_loss',\n",
    "#     max_trials=4,\n",
    "#     executions_per_trial=2,\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "\n",
    "# # Run the hyperparameter search\n",
    "# tuner.search(\n",
    "#     x=x_train_reshaped,\n",
    "#     y=y_train.values.reshape(-1,1),\n",
    "#     epochs=30,\n",
    "#     batch_size=32,\n",
    "#     validation_data=(x_test_reshaped, y_test.values.reshape(-1, 1)),\n",
    "#     callbacks=[early_stopping],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "moe_model.fit(x_train_reshaped_lstm, y_train.values.reshape(-1,1),#\n",
    "              validation_data=(x_test_reshaped_lstm, y_test.values.reshape(-1, 1)),#\n",
    "                epochs=30, batch_size=32,callbacks=[early_stopping,reduce_lr],verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_model.evaluate(x_val_reshaped, y_val.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prob = moe_model.predict(x_val_reshaped_lstm)\n",
    "y_pred_moe = np.argmax(predicted_prob, axis=1)\n",
    "#y_pred_moe = moe_model.predict(x_val_reshaped_lstm)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix( y_val.values.reshape(-1,1), y_pred_moe.reshape(-1,1))# for lstm y_val.values.reshape(-1,1), y_pred.reshape(-1,1) #for cnn y_val, y_pred\n",
    " \n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', annot_kws={\"size\": 12})\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_val, y_pred_moe))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
